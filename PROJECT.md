# Project Atom

> 영화 *리얼 스틸*의 아톰에서 영감을 받아, AI 복싱 트레이너에서 시작해 인간 동작 이해 플랫폼으로 확장한다.

---

## 1. 동기

데이터 엔지니어 4년차. 데이터가 대시보드 위의 숫자에 머무는 것이 아닌, 물리적 세계와 인터렉션하고 가시적 임팩트를 만드는 경험을 하고 싶었다. 리얼 스틸의 아톰이 떠올랐고, 스파링 로봇을 만들기 시작했다.

프로젝트를 진행하며 도달한 인사이트:

1. **진짜 가치는 하드웨어가 아니라 소프트웨어에 있다.** 영상에서 사람의 동작을 이해하는 엔진을 잘 만들면, 하드웨어 적용은 그 다음 문제다.
2. **복싱 수련에 실질적으로 도움이 되어야 한다.** 선수든 일반인이든, 실제 훈련에 쓸 수 있는 도구를 만든다.
3. **이 기술은 복싱에 머물 필요가 없다.** 동일한 엔진이 다른 도메인에 적용될 수 있다.

---

## 2. 핵심 목표: AI 복싱 트레이너

### 만들려는 것

웹캠 하나로 동작하는 AI 복싱 트레이너. 실제 트레이너의 **패드 워크**를 소프트웨어로 구현한다.

실제 패드 워크에서 트레이너가 하는 일:
- 콤보를 지시한다 ("잽-크로스-훅!")
- 가끔 공격을 던져 방어를 유도한다 ("슬립!")
- 실행의 정확도/타이밍을 평가한다
- 수준에 맞게 난이도를 조절한다

**이것을 카메라 기반으로 구현한다.** "맞는 느낌"은 재현할 수 없지만, 복싱 실력의 핵심인 **타이밍, 반응 속도, 패턴 인식**은 화면만으로 훈련 가능하다.

### 가치

혼자 섀도우 복싱하면 피드백이 없다. 트레이너에게 패드 워크를 받으려면 시간과 비용이 든다. AI 복싱 트레이너는 이 사이의 공백을 채운다 — 혼자서도 구조화된 훈련과 실시간 피드백을 받을 수 있다.

### 왜 복싱인가

- 자연적으로 제약된 환경 (단일 인물, 상체, 유한 동작) → 가장 깔끔한 테스트베드
- 이산적이고 잘 정의된 동작 체계
- 경기 영상이 풍부 → YouTube에 수만 개의 공개 데이터
- **격투기 영상 분석 시장에 공백** → 농구(Second Spectrum), 축구(StatsBomb)는 있지만 격투기는 없다
- 리얼 스틸의 아톰 — 만들고 싶은 걸 만들어야 끝까지 간다

---

## 3. Phase 로드맵

최종 목표(AI 복싱 트레이너)에서 역순으로 설계했다. 모든 Phase가 다음 Phase의 직접적 기반이 된다.

### Phase 1: 스파링의 눈 — 실시간 동작 이해

**왜 이게 먼저인가:** 트레이너가 사용자를 훈련시키려면 먼저 사용자가 뭘 하고 있는지 알아야 한다.

**목표:** 웹캠 영상에서 복싱 동작을 실시간으로 이해한다. 단일 프레임 분류가 아니라, 시간적 패턴(궤적, 콤보)을 인식하는 시퀀스 모델까지.

**완료 기준:**
- 5개 동작(guard, jab, cross, lead_hook, slip) 실시간 분류, 정확도 ≥ 80%
- 시퀀스 모델이 동작하여 콤보 감지 가능 (잽→크로스 구분)
- <500ms 추론 지연

| 단계 | 태스크 | → Phase 2에서 | 상태 |
|------|--------|-------------|------|
| **1A** | 데이터 촬영 (5→15→45영상) | 학습 데이터 기반 | ⬜ |
| **1B** | 키포인트 추출 + 시각적 검증 | 경기 영상에도 재사용 | ⬜ |
| **1C** | 전처리 → Parquet 데이터셋 | 스키마 재사용 | ⬜ |
| **1D** | RF 베이스라인 (파이프라인 sanity check) | — | ⬜ |
| **1E** | LSTM / ST-GCN 시퀀스 모델 | 콤보 감지의 기반 | ⬜ |
| **1F** | 실시간 추론 데모 | 트레이너의 눈 | ⬜ |

**이미 완료된 것:**
- MediaPipe 포즈 추출 (130+ FPS) ✅
- 분류기 프로토타입 (`move_classifier.py`) ✅
- JSONL 기록기 (`recorder.py`) ✅

**모델 전략:** RF로 파이프라인을 검증한 뒤 시퀀스 모델(LSTM → ST-GCN)로 넘어간다. Phase 2-3에서 콤보 감지, 타이밍 분석, 의도 예측이 모두 시퀀스 모델 위에서 동작하기 때문에 Phase 1에서 시퀀스 모델까지 가는 것이 필수.

**주간 계획:** W1: 5영상+RF sanity check → W2: 나머지 영상+LSTM → W3-4: ST-GCN+실시간 데모

### Phase 2: 스파링의 몸 — 트레이너 MVP

**왜 이게 두 번째인가:** 눈이 있으니 이제 기본적인 훈련 루프를 만든다. 가장 단순한 형태라도 루프가 도는 것이 핵심.

**목표:** 기본적인 AI 패드 워크. 지시 → 실행 → 피드백 루프가 동작한다.

**완료 기준:**
- 공격 훈련: 콤보 지시 → 사용자 실행 → 정확도/타이밍 피드백
- 방어 훈련: 가상 상대 공격 모션 → 사용자 방어 → 반응 속도 측정
- 기본 세션 흐름 (워밍업 → 공격 드릴 → 방어 드릴 → 쿨다운)

**공격 훈련 시나리오:**
```
시스템: "잽!"
사용자: 잽을 던짐
시스템: ✅ 잽 인식. 0.3초. 가드 복귀 양호.
시스템: "잽-크로스!"
사용자: 잽-크로스
시스템: ✅ 콤보 인식. 크로스가 0.1초 느림. 가드가 내려갔다.
```

**방어 훈련 시나리오:**
```
화면: 가상 상대가 훅 모션을 보여줌
사용자: 덕으로 회피
시스템: ✅ 덕 인식. 반응 시간 0.4초. 
화면: 가상 상대가 잽을 던짐
사용자: 슬립
시스템: ✅ 슬립 인식. 반응 시간 0.35초.
```

| 단계 | 태스크 | → Phase 3에서 | 상태 |
|------|--------|-------------|------|
| **2A** | 공격 드릴 엔진 (콤보 지시 + 평가) | 적응형 지시의 기반 | ⬜ |
| **2B** | 방어 드릴 엔진 (상대 공격 + 방어 인식) | 상대 모델의 기반 | ⬜ |
| **2C** | 가상 상대 시각화 (골격 또는 간단한 캐릭터) | UI 재사용 | ⬜ |
| **2D** | 세션 흐름 + 결과 리포트 | 세션 데이터 축적 | ⬜ |
| **2E** | 동작 클래스 9개 확장 | 더 풍부한 스파링 | ⬜ |

### Phase 3: 스파링의 뇌 — 똑똑한 트레이너

**왜 이게 세 번째인가:** 루프가 도니까 이제 트레이너를 똑똑하게 만든다. 고정 패턴이 아니라 사용자에게 맞춤화되고, 실제 파이터의 패턴을 학습한 상대.

**목표:** 경기 영상에서 학습한 패턴 기반의 적응형 트레이너.

**완료 기준:**
- 경기 영상 분석 → 파이터 프로필 자동 구축
- 프로필 기반 상대 ("타이슨처럼 훈련해줘")
- 사용자 약점 자동 분석 → 약점 집중 드릴
- 실력 성장에 따른 적응형 난이도

| 단계 | 태스크 | 상태 |
|------|--------|------|
| **3A** | 경기 영상 다중 인물 포즈 추출 | ⬜ |
| **3B** | 경기 자동 분석 (라운드별 통계, 콤보 패턴) | ⬜ |
| **3C** | 파이터 프로필 구축 (습관, 선호 콤보, 약점) | ⬜ |
| **3D** | 프로필 기반 상대 행동 모델 | ⬜ |
| **3E** | 사용자 프로필 축적 → 약점 분석 → 맞춤 드릴 | ⬜ |
| **3F** | 적응형 난이도 (반응 속도, 패턴 복잡도) | ⬜ |

**스트레치 골:** 규칙 기반 → 학습된 정책(RL). 시뮬레이션에서 학습, 소프트웨어에서 실행.

### Phase 4+: 확장

| 방향 | 내용 |
|------|------|
| **범용 플랫폼** | 동작 이해 엔진을 다른 도메인에 적용 |
| **로봇** | 소프트웨어 트레이너를 물리적 로봇에 탑재. 진짜 아톰. |

---

## 4. 플랫폼 트랙: 범용 동작 이해

### 비전

복싱에서 만든 동작 이해 엔진을 범용화한다. Config를 바꾸는 수준이 아니라, **모델 자체가 인간의 동작을 이해하는 범용 능력**을 가지는 것이 목표. 피겨 영상을 주면 "트리플 악셀"을, 태권도 영상을 주면 "돌려차기"를 인식하는 수준.

### 발전 경로

```
지금: 복싱 5개 동작, 직접 촬영, 지도 학습
  ↓
다음: NTU RGB+D, Kinetics-Skeleton 등 대규모 공개 데이터셋
      수십~수백 개 동작 클래스
  ↓
그 다음: 사전학습된 골격 표현 (pre-trained skeleton representation)
         새 도메인 동작을 few-shot으로 인식
  ↓
장기: 범용 인간 동작 이해 모델
```

### 확장성 설계 원칙 (지금부터 적용)

| 원칙 | 구현 |
|------|------|
| **동작 클래스 외부화** | `config.yaml`에 정의. 코드에 복싱 용어 하드코딩 금지. |
| **키포인트 선택 유연성** | 상체 9개(복싱), 전신(피겨) 등 설정으로 변경 |
| **도메인 독립 데이터 스키마** | Parquet 메타데이터에 도메인, 클래스, 키포인트 정의 포함 |
| **원본 영상 영구 보존** | 추정기/전처리 변경 시 재처리 가능 |
| **모듈형 파이프라인** | 추출/전처리/학습/추론 각 단계 독립 교체 |

---

## 5. 아키텍처 결정

### 5.1 동작 분류 (각도 미러링 아님)

| 접근법 | 결론 |
|--------|------|
| 각도 미러링 | Z축이 X/Y보다 3배 노이즈. ❌ |
| 휴리스틱 보정 | 노이즈와 싸우는 구조. ❌ |
| **동작 분류** | Z-노이즈 회피. 깔끔한 출력. 확장 가능. ✅ |

### 5.2 영상 기반, 추가 센서 없음

- 웹캠, 스마트폰, 녹화 영상 — 어떤 영상 소스든 입력 가능
- 인터넷 영상 데이터가 풍부 → 학습 데이터 확보 용이 (특히 Phase 3 경기 분석)
- 센서 무의존 = 최대 배포 범위
- 향후 IMU 보조 가능 — 필수가 되어서는 안 됨

### 5.3 키포인트 기반 (원본 픽셀 아님)

- 동작 = 골격 패턴. 외형에 자연적 불변.
- 수천 개 샘플 충분. CPU 실시간 가능.
- 해석 가능 → 피드백("팔꿈치가 열렸다")의 근거.
- 도메인 변경 시 동작 클래스만 재정의.

---

## 6. 모델 아키텍처

### 포즈 추정: MediaPipe

33 키포인트, 상대적 Z좌표, CPU 30+ FPS. 빠른 동작 시 지터 → 스무딩으로 완화.

### 분류 모델 발전

| 모델 | 역할 | Phase |
|------|------|-------|
| **RF** | 파이프라인 sanity check | P1 초반 |
| **LSTM** | 시퀀스 모델 진입 | P1 중반 |
| **ST-GCN** | 골격 토폴로지 + 시공간 모델링. 콤보 인식. | P1 후반 ~ P2 |

**왜 시퀀스 모델이 Phase 1에서 필수인가:** Phase 2의 콤보 지시/평가, Phase 3의 패턴 분석이 모두 시간적 패턴 위에서 동작한다. RF(단일 프레임)로 멈추면 Phase 2에서 처음부터 다시 시작해야 한다.

### 추론 파이프라인

```
영상 (30fps)
  → MediaPipe (33kp × [x,y,z,vis])
  → 스무딩 → 키포인트 선택 (config) → 정규화
  → 슬라이딩 윈도우 (30f, stride 5)
  → 시퀀스 모델 → 동작 + 신뢰도
  → 트레이너 엔진 (지시 / 평가 / 반응)
```

---

## 7. 데이터 파이프라인

### 아키텍처

```
[수집]     →  [추출]       →  [전처리]     →  [저장]    →  [로딩]
 원본 영상     키포인트        정규화          Parquet     모델별
 + 라벨       (.npy)         스무딩/윈도잉               DataLoader
```

변경 영향 격리: 추정기 교체 → 추출만. 전처리 변경 → 전처리만. 모델 변경 → DataLoader만.

### 수집

**Phase 1 (직접 촬영):** 5개 동작. 스마트폰 720p 30fps, 1.5-2m. 앵글(3) × 속도(3) × 피험자. 파일명 = 라벨 (`{subject}_{action}_{angle}_{speed}.mp4`). 우선순위: 5영상 → 15 → 45 → 2번째 피험자.

**Phase 3 (경기 영상):** YouTube 공개 경기 영상. 다중 인물 처리. 프로필 구축용.

**플랫폼 트랙:** NTU RGB+D, Kinetics-Skeleton 등 공개 벤치마크.

### 전처리

33→9 키포인트 → visibility 마스킹(<0.5) → NaN 보간 → 힙 중심 정규화 + 어깨 스케일 → Savitzky-Golay 스무딩 → 슬라이딩 윈도우(30f, stride 5) → NaN 30% 초과 제거.

### 저장

Parquet + 메타데이터 JSON. 피험자 단위 분할. 증강은 DataLoader 레벨(좌우 반전, 시간 스케일, 노이즈, 랜덤 크롭).

---

## 8. 기술 스택

| 레이어 | 현재 | Phase 2+ |
|--------|------|----------|
| 포즈 추정 | MediaPipe (33kps, 130+ FPS) | + 다중 인물 지원 |
| 전처리 | Savitzky-Golay, 힙 정규화 | One Euro Filter |
| 분류 | RF → LSTM → ST-GCN | ST-GCN + 콤보 모델 |
| 데이터 | Parquet, JSONL | + DuckDB |
| 학습 | CPU / Colab T4 | Colab Pro / Vast.ai |
| ML/RL | scikit-learn, PyTorch | + Stable-Baselines3 |
| UI | — | 웹 (Phase 2) |

---

## 9. 디렉토리 구조

```
atom/
├── configs/                          # 도메인 설정 (동작 클래스, 키포인트)
├── data/
│   ├── raw/                          # 원본 영상 (영구 보관)
│   ├── keypoints/                    # 추출 (.npy + meta.json)
│   └── processed/                    # 학습용 (Parquet + metadata.json)
├── src/
│   ├── extraction/pose_extractor.py  # 포즈 추정 래퍼 (교체 가능)
│   ├── preprocessing/pipeline.py
│   ├── dataset/{builder,loader}.py
│   ├── models/{random_forest,lstm,stgcn}.py
│   ├── trainer/                      # Phase 2: AI 트레이너
│   │   ├── drill_engine.py           # 공격/방어 드릴 관리
│   │   ├── evaluator.py              # 실행 평가 (정확도, 타이밍)
│   │   └── session.py                # 세션 흐름 관리
│   ├── analysis/                     # Phase 3: 경기 분석
│   │   ├── fight_analyzer.py         # 경기 영상 자동 분석
│   │   └── profile_builder.py        # 파이터 프로필 구축
│   ├── opponent/                     # Phase 3: 상대 모델
│   │   ├── behavior_model.py         # 프로필 기반 행동
│   │   └── adaptive.py               # 적응형 난이도
│   ├── move_classifier.py            # 실시간 추론
│   └── evaluation/recorder.py
├── docs/engineering_log.md
└── Makefile
```

---

## 10. 엔지니어링 교훈

1. **MediaPipe Z축 불신** — 2D + 시간 패턴으로 대체
2. **분류 > 미러링** — 이산 분류가 연속 복사보다 깔끔
3. **시퀀스 모델이 필수** — 단일 프레임으로는 콤보/타이밍/의도를 모름
4. **One Euro Filter** — 적응형 스무딩
5. **키포인트 > 픽셀** — 외형 불변, 적은 데이터/연산, 해석 가능
6. **피험자 단위 분할** — 데이터 리크 방지
7. **파일명 = 라벨** — 어노테이션 오버헤드 제거

---

## 11. 이 프로젝트가 아닌 것

- **실제 스파링이 아니다** — 타격 임팩트는 없다. AI 패드 워크 트레이너에 가깝다.
- **아직 범용 플랫폼이 아니다** — 비전은 범용, 지금 스코프는 복싱.
- **로봇 프로젝트가 아니다 (지금은)** — 소프트웨어 우선. 로봇은 장기 비전.
- **제품이 아니다 (아직)** — 기술 검증 + 포트폴리오. 가치가 확인되면 제품화.

---

## 12. 하드웨어 현황 (보류 중)

소프트웨어에 집중하기 위해 보류. 소프트웨어 트레이너가 충분히 성숙하면 로봇에 탑재한다.

```
완료:
  ✅ ESP32 + PCA9685 + 7x MG996R 서보 연결/스위프/시리얼/DTR
  ✅ 동작 재생기 프로토타입 (move_player.py)

보류:
  ⬜ 프레임 조립, 관절 매핑, 보정
  ⬜ 하드웨어 통합 (영상→분류→서보)
```

---

*프로젝트 시작: 2026 · 관리자: Charlie*
*아키텍처: 동작 분류 (2026-02-08 결정)*
*방향: AI 복싱 트레이너 → 범용 동작 이해 플랫폼 (2026-02-21)*