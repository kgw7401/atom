# Project Atom — 영상 기반 인간 동작 이해 플랫폼

> 영화 *리얼 스틸*의 아톰에서 영감을 받아, 복싱 스파링 로봇으로 시작해 인간 동작 이해의 범용 플랫폼을 구축한다.

---

## 1. 동기

데이터가 대시보드 위의 숫자에 머무는 것이 아닌, 물리적 세계와 인터렉션하고 가시적 임팩트를 만드는 경험을 하고 싶었다. UAM, 자율주행, 로보틱스를 탐색하던 중 리얼 스틸의 아톰이 떠올랐고, 스파링 로봇을 만들기 시작했다.

프로젝트를 진행하면서 도달한 인사이트: **진짜 가치는 하드웨어가 아니라 소프트웨어에 있다.** 영상에서 사람의 동작을 이해하고 분류하는 "인지 엔진"을 잘 만들면, 하드웨어 적용은 그 다음 문제다. 그리고 이 엔진은 복싱에만 머물 필요가 없다.

---

## 2. 비전: 계층적 확장

이 프로젝트의 핵심 역량은 **"영상에서 골격 시퀀스를 추출하고, 시간적 패턴으로 인간의 동작과 의도를 파악하는 것"**이다. 이 역량은 복싱을 넘어 인간 동작 이해가 필요한 모든 시스템에 적용 가능하다.

```
┌─────────────────────────────────────────────────────────────┐
│  인간 의도 이해 시스템                                           │
│  FSD, VLA 로봇 협업, 작업자 의도 파악 (장기 비전)                    │
│  ┌───────────────────────────────────────────────────┐      │
│  │  범용 스포츠/피트니스 동작 분석 (다음 확장)                 │      │
│  │  ┌───────────────────────────────────┐            │      │
│  │  │  🥊 복싱 동작 인식 + 로봇 (현재)        │            │      │
│  │  └───────────────────────────────────┘            │      │
│  └───────────────────────────────────────────────────┘      │
└─────────────────────────────────────────────────────────────┘
```

| 레이어 | 내용 | 핵심 문제 |
|--------|------|----------|
| **코어** | 복싱 동작 인식 + 스파링 로봇 | "이 사람이 잽을 던지는가?" |
| **확장 1** | 스포츠/피트니스 동작 분석 | "이 동작의 폼이 올바른가?" |
| **확장 2** | FSD, VLA, 협동 로봇 | "이 사람이 무엇을 하려 하는가?" |

**확장의 논리:** 복싱에서 "잽 궤적을 인식한다"와 자율주행에서 "보행자가 길을 건너려 한다를 예측한다"는 구조적으로 같은 문제다 — 골격 시퀀스의 시간적 패턴에서 인간의 동작과 의도를 읽는 것. 환경의 복잡도(통제된 실내 → 야외, 단일 → 다중 인물, 폐색)는 다르지만 핵심 역량은 동일하다.

**지금은 복싱에 집중한다.** 단, 모든 아키텍처 결정은 확장을 염두에 둔다. 복싱에 하드코딩되는 것은 동작 클래스 정의와 반응 규칙뿐이어야 한다.

### 왜 복싱이 첫 도메인인가

- 자연적으로 제약된 환경 (단일 인물, 상체, 유한 동작) → 가장 깔끔한 테스트베드
- 이산적이고 잘 정의된 동작 체계 → 모호한 경계가 적다
- 상체 로봇 데모의 시각적 임팩트
- 리얼 스틸의 아톰 — 만들고 싶은 걸 만들어야 끝까지 간다

---

## 3. 핵심 구조: 인지 엔진 + 어플리케이션

```
┌──────────────────────────────────────────────────────────────┐
│  어플리케이션 레이어 (도메인 특화)                              │
│  ┌──────────────┐  ┌──────────────┐  ┌───────────────────┐  │
│  │ 스파링 로봇   │  │ 홈 트레이너   │  │ [미래 어플리케이션] │  │
│  │ (하드웨어)    │  │ (웹/모바일)   │  │                   │  │
│  └──────┬───────┘  └──────┬───────┘  └────────┬──────────┘  │
├─────────┴─────────────────┴────────────────────┴─────────────┤
│  인지 엔진 (도메인 독립)                                      │
│  영상 → 포즈 추정 → 키포인트 전처리 → 시퀀스 모델 → 동작 분류  │
└──────────────────────────────────────────────────────────────┘
```

**인지 엔진**이 핵심 자산이다. "영상 → 인간이 무슨 동작을 하는가"를 답한다. 어플리케이션 레이어는 그 답으로 무엇을 할지 결정한다.

### 확장성을 위한 설계 원칙

| 원칙 | 구현 |
|------|------|
| **동작 클래스 외부화** | `config.yaml`에 정의. 코드에 복싱 용어 하드코딩 금지. |
| **키포인트 선택 유연성** | 상체 9개(복싱), 하체 포함(골프) 등 설정으로 변경 |
| **도메인 독립 데이터 스키마** | Parquet 메타데이터에 도메인, 클래스, 키포인트 정의 포함 |
| **평가 프레임워크 표준화** | 수집 → 학습 → 평가가 동일 인터페이스로 동작 |
| **원본 영상 영구 보존** | 추정기/전처리 변경 시 재처리 가능 |

---

## 4. 아키텍처 결정

### 4.1 동작 분류 + 재생

| 접근법 | 결론 |
|--------|------|
| 각도 미러링 | Z축이 X/Y보다 3배 노이즈. 전방 펀치 인식 불가. ❌ |
| 휴리스틱 보정 | 근본적으로 노이즈와 싸우는 구조. ❌ |
| **동작 분류 + 재생** | Z-노이즈 회피. 깔끔한 출력. 반응형 확장 가능. ✅ |

```
영상 ──▶ 2D 포즈 ──▶ 동작 분류기 ──▶ 의사결정 ──▶ 출력
         (MediaPipe)   (무슨 동작?)    (뭘 할까?)
```

Phase별 의사결정 교체: 미러링(P1) → 카운터(P2) → 학습된 정책(P3).

### 4.2 영상 기반, 추가 센서 없음

- **배포 장벽 제로:** 웹캠, 스마트폰, 녹화 영상, CCTV — 어떤 영상 소스든 입력 가능
- **데이터 확보 용이:** 인터넷 데이터의 대부분이 영상. 학습 데이터 소싱에 유리.
- **센서 무의존 = 최대 배포 범위:** 확장 시 추가 인프라 불필요
- 향후 IMU 보조 가능 — 필수가 되어서는 안 됨

### 4.3 키포인트 기반 (원본 픽셀 아님)

- 동작 = 골격 패턴. 외형(옷, 조명, 체형)에 자연적 불변.
- 수천 개 샘플 충분 (이미지 기반은 수만 개). CPU 실시간 가능.
- 해석 가능 (관절 각도/궤적으로 분류 근거 설명).
- **도메인 변경 시 동작 클래스만 재정의** — 이미지 데이터셋 재수집 불필요.

---

## 5. 모델 아키텍처

### 포즈 추정: MediaPipe

33 키포인트(손 기준점 포함), 상대적 Z좌표, CPU 30+ FPS, 모바일 배포 가능. 빠른 동작 시 지터 → One Euro / Savitzky-Golay 스무딩.

### 분류 모델 발전

| 단계 | 모델 | 환경 |
|------|------|------|
| 프로토타입 | RF / LSTM | CPU |
| 목표 | **ST-GCN** | Colab T4 |
| 미래 | MS-G3D / CTR-GCN | GPU |

**왜 ST-GCN:** 복싱 동작은 시간에 따른 관절 관계. Spatial(골격 토폴로지) + Temporal(동작 역학)을 자연스럽게 모델링. 단일 프레임으로는 잽/크로스 구분 불가 — 15-30 프레임의 궤적이 필요.

### 추론 파이프라인

```
영상 (30fps)
  → MediaPipe (33kp × [x,y,z,vis])
  → 스무딩 → 키포인트 선택 (9개, config) → 정규화 (힙 중심 + 어깨 스케일)
  → 슬라이딩 윈도우 (30f, stride 5) → ST-GCN → 동작 + 신뢰도
  → 의사결정 모듈
```

---

## 6. 데이터 파이프라인

### 아키텍처

```
[수집]     →  [추출]       →  [전처리]     →  [저장]    →  [로딩]
 원본 영상     키포인트        정규화          Parquet     모델별
 + 라벨       (.npy)         스무딩/윈도잉               DataLoader
```

변경 영향 격리: 추정기 교체 → 추출만. 전처리 변경 → 전처리만. 모델 변경 → DataLoader만.

### 수집

**동작 (Phase 1: 5개):** guard, jab, cross, lead_hook, slip → 검증 후 9개 확장.

**촬영:** 스마트폰 720p 30fps, 1.5-2m. 변동: 앵글(45°/정면/-45°) × 속도(3단계) × 피험자. 45영상, 영상당 20-30회. 파일명 = 라벨 (`{subject}_{action}_{angle}_{speed}.mp4`).

**우선순위:** 5영상(검증) → 15(속도) → 45(전체) → 2번째 피험자(일반화).

### 전처리

33→9 키포인트 선택 → visibility 마스킹(<0.5) → NaN 보간 → 힙 중심 정규화 + 어깨 스케일 → Savitzky-Golay 스무딩 → 슬라이딩 윈도우(30f, stride 5) → NaN 30% 초과 제거.

### 저장

Parquet + 메타데이터 JSON. 피험자 단위 분할. 증강은 DataLoader 레벨(좌우 반전, 시간 스케일, 노이즈, 랜덤 크롭).

---

## 7. 어플리케이션

### 7.1 스파링 로봇

```
Mac (M3 Pro) → USB Serial → ESP32 → I2C → PCA9685 → 7x MG996R
  0-2: L_shoulder_tilt/pan, L_elbow
  3-5: R_shoulder_tilt/pan, R_elbow
  6: Torso_yaw
```

하드웨어 발전: MG996R(현재) → Dynamixel(P2-3) → 브러시리스(P3+). 소프트웨어가 정당화할 때까지 업그레이드하지 않는다.

### 7.2 홈복싱 트레이너

인지 엔진의 두 번째 어플리케이션. 웹캠만으로 동작. 실시간 자세 피드백, 콤비네이션 채점, 운동 지표 추적, 난이도 조절. FightCamp/Liteboxer 존재하나 카메라 기반 포즈 피드백은 공백.

---

## 8. Phase 로드맵

### Phase 1: 섀도우 모드 MVP — 진행 중

**목표:** 5개 동작 인식 + 미러링. <500ms 지연. 정확도 ≥ 80%.

| 단계 | 태스크 | 상태 |
|------|--------|------|
| **1A: HW** | ESP32+PCA9685+7서보 연결/스위프/시리얼/DTR | ✅ |
| | 프레임 조립, 관절 매핑, 보정, 가드 홈포지션 | ⬜ |
| **1B: 데이터** | 촬영(5→15→45), 추출, 검증, 전처리, RF 베이스라인 | ⬜ |
| **1C: 모델** | MediaPipe 포즈(130+FPS), 분류기/재생기 프로토타입 | ⬜ |
| | LSTM → ST-GCN, 비교, 임계값 튜닝 | ⬜ |
| **1D: 통합** | 모션 키프레임, 영상→분류→서보, 지연 측정 | ⬜ |
| **1E: 기록** | JSONL 기록기 완료, 타임스탬프+신뢰도 로깅 | ⬜ |

### Phase 2: 반응 모드

**목표:** 카운터 펀칭. 적절한 반응 ≥ 70%.

| 단계 | 내용 |
|------|------|
| **2A** | 반응 규칙 (Jab→슬립+카운터, Hook→덕 등), <300ms |
| **2B** | 9개 클래스 확장, 콤보 시퀀스 감지 |
| **2C** | 난이도 단계 (반응 속도, 텔레그래프) |
| **2D** | 홈 트레이너: 폼 스코어링, 콤비네이션 훈련, 웹/모바일 UI |

### Phase 3: 학습된 반응

**목표:** 규칙 → 학습 정책. 상대 적응. 정책이 규칙 초과.

| 단계 | 내용 |
|------|------|
| **3A** | MuJoCo 시뮬레이션, 보상 설계 |
| **3B** | PPO/SAC, 상대 커리큘럼, 셀프플레이 |
| **3C** | Sim2Real: 도메인 랜덤화, 시스템 식별, 보정 |
| **3D** | VLA 확장: LLM 통합, 학습된 동작 생성 (스트레치 골) |

---

## 9. 기술 스택

| 레이어 | 현재 | Phase 2+ |
|--------|------|----------|
| 포즈 추정 | MediaPipe (33kps, 130+ FPS) | + 뎁스 (필요시) |
| 전처리 | Savitzky-Golay, 힙 정규화 | One Euro Filter |
| 분류 | RF / LSTM | ST-GCN → MS-G3D |
| 데이터 | Parquet, JSONL | + DuckDB |
| 시뮬레이션 | PyBullet | MuJoCo |
| 제어 | Python → ESP32 → PCA9685 | ROS2 (필요시) |
| 학습 | CPU / Colab T4 | Colab Pro / Vast.ai |
| ML/RL | scikit-learn, PyTorch | + Stable-Baselines3 |

---

## 10. 디렉토리 구조

```
atom-boxing/
├── configs/                          # 도메인 설정 (동작 클래스, 키포인트, 파이프라인)
├── data/
│   ├── raw/                          # 원본 영상 (영구 보관)
│   ├── keypoints/                    # 추출 (.npy + meta.json)
│   └── processed/                    # 학습용 (Parquet + metadata.json)
├── src/
│   ├── collection/collector.py
│   ├── extraction/pose_extractor.py  # MediaPipe 래퍼 (교체 가능)
│   ├── preprocessing/pipeline.py
│   ├── dataset/{builder,loader}.py
│   ├── models/{random_forest,lstm,stgcn}.py
│   ├── move_classifier.py            # 실시간 추론
│   ├── move_player.py                # 서보 모션 재생
│   └── evaluation/recorder.py
├── hardware/esp32/
├── docs/engineering_log.md
└── Makefile
```

---

## 11. 엔지니어링 교훈

1. **MediaPipe Z축 불신** — 2D + 시간 패턴으로 대체
2. **분류 > 미러링** — 이산 분류가 연속 복사보다 깔끔
3. **One Euro Filter** — 적응형 스무딩 (정지↔이동 밸런스)
4. **워치독 + DTR** — ESP32 초기화 타이밍 주의
5. **키포인트 > 픽셀** — 외형 불변, 적은 데이터/연산, 해석 가능
6. **피험자 단위 분할** — 데이터 리크 방지
7. **파일명 = 라벨** — 어노테이션 오버헤드 제거

---

## 12. 이 프로젝트가 아닌 것

- **아직 범용 플랫폼이 아니다** — 비전은 범용, 스코프는 복싱
- **제품이 아니다** — 연구 프로토타입 (홈 트레이너는 제품 가능)
- **안전한 스파링 파트너가 아니다** — MG996R은 실제 펀치 불가
- **실시간 RL이 아니다** — 시뮬레이션 학습, 하드웨어는 실행만
- **완전한 휴머노이드가 아니다** — 상체만, 고정 베이스

---

*프로젝트 시작: 2026 · 관리자: Charlie*
*아키텍처: 동작 분류 + 재생 (2026-02-08)*
*PRD v4: 인지 엔진 중심, FSD/VLA 확장 비전 (2026-02-20)*